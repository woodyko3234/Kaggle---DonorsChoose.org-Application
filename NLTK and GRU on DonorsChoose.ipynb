{
  "cells": [
    {
      "metadata": {
        "_uuid": "8148d27c795b680a75e1f43092caebc1b8d9257f"
      },
      "cell_type": "markdown",
      "source": "After take a look into the data distribution of DonorsChoose.org, I found that I couldn't build a simple model that applied features such as grades, subject categories, states, and number of previously posted projects, to make precise prediction of whether the project would get approved or not. So now I'm going to dig into how the project title and project essays make people decide to support it."
    },
    {
      "metadata": {
        "_uuid": "39793227486c5e29b163e266199e6c18302ff0cc"
      },
      "cell_type": "markdown",
      "source": "### Load data and import modules"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "141b475b114ac2ee3366e467b406d21a2689cab9",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#load modules\nimport nltk #for tokenizing\nimport tqdm\n#https://github.com/tqdm/tqdm 第三方進度條模組\nimport numpy as np\nimport pandas as pd\n#PunktSentenceTokenizer\nnltk.download(\"punkt\")\nimport zipfile\nimport gc",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": true,
        "_uuid": "a6d32a5e5f563b9927bea5eefc26d418b3206757"
      },
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c96ead0558a26fc228b51a0b9c4ce9fdfada67d8",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "import os\nprint(os.listdir(\"../input/glove6b300dtxt\"))\nos.listdir(\"../input/donorschoose-application-screening\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2b3b88229ae38fa682c6493c010e12021e4e3849",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#load data\ndf_train = pd.read_csv('../input/donorschoose-application-screening/train.csv')\ndf_test = pd.read_csv('../input/donorschoose-application-screening/test.csv')\ndf_resources = pd.read_csv('../input/donorschoose-application-screening/resources.csv')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4820a58e098793d8c8d7150e06ecd056ff38bdd6"
      },
      "cell_type": "markdown",
      "source": "### Data types and columns\n\n**Text**: project_title, project_essay_1, project_essay_2, project_essay_3, project_essay_4, project_resource_summary in **train/test**, and description in **resources**\n\n**Class/Label**: teacher_id, teacher_prefix, project_grade_category, project_subject_categories, project_subject_subcategories in **train/test**\n\n**Quantity**: quantity and price in **resources**, and teacher_number_of_previously_posted_projects in **train/test**"
    },
    {
      "metadata": {
        "_uuid": "fb978ac3d22608c2750b33a4d7e4b2fe928b15fa"
      },
      "cell_type": "markdown",
      "source": "## Deal with Text data"
    },
    {
      "metadata": {
        "_uuid": "93ce2c9cc4c4d202190f9be3222da5a1b0d84431"
      },
      "cell_type": "markdown",
      "source": "#### 1. Load the pre-trained embedded word vectors"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": true,
        "_uuid": "7b07ae7cc99566bbfdc1547b38779bf7c892b256"
      },
      "cell_type": "code",
      "source": "#load word embedded vectors\nimport os\nembedding_path = \"../input/glove6b300dtxt/glove.6B.300d.txt\"\n\ndef read_embedding_vec(embedding_file_path):\n    embedding_list = []\n    embedding_word_dict = dict()\n    \n    f = open(embedding_file_path)\n\n    for index, line in enumerate(f):\n        #the line contains target word and its word vector, so split it first\n        values = line.split()\n        word = values[0]\n        try:\n            coefs = np.asarray(values[1:], dtype='float32')\n        except:\n            continue\n        #save the word vec into a list\n        embedding_list.append(coefs)\n        #build a connection between word and its word id (0, 1, 2, ......)\n        embedding_word_dict[word] = len(embedding_word_dict)\n    f.close()\n    embedding_list = np.array(embedding_list)\n    return embedding_list, embedding_word_dict",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": true,
        "_uuid": "41f7dcc3881f89e228027e7bf5c7c6f9424d9977"
      },
      "cell_type": "code",
      "source": "#load embedding word vectors\nembedding_list, embedding_word_dict = read_embedding_vec(embedding_path)\n#embedding_list[:5], embedding_word_dict['the']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": true,
        "_uuid": "d0de142f6ea0267fbec814f105263674164d00ff"
      },
      "cell_type": "code",
      "source": "#set word_vector for UNKNOWN_WORD and _NAN_\ntry:\n    embedding_word_dict['nanword']\nexcept:\n    embedding_word_dict['nanword'] = len(embedding_word_dict)\n    embedding_list = np.append(embedding_list, [np.zeros_like(embedding_list[0,:])], axis = 0)\ntry:\n    embedding_word_dict['unknownword']\nexcept:\n    embedding_word_dict['unknownword'] = len(embedding_word_dict)\n    embedding_list = np.append(embedding_list, [np.zeros_like(embedding_list[0,:])], axis = 0)\n#(embedding_word_dict['nanword'], embedding_word_dict['unknownword'], len(embedding_word_dict)), embedding_list[-3:]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8c12d3c926f44562471a53f2fb3f70b06667901e"
      },
      "cell_type": "markdown",
      "source": "Now we have loaded the pre-trained word vectors, the next step is to transform our project titles and essays. Note that the word embedding does not include \"can't\", \"couldn't\", \"wouldn't\", and things like those, we need to separate the not things when doing tokenization."
    },
    {
      "metadata": {
        "_uuid": "eb66279952e4ff43de14102bb5562743c462899e"
      },
      "cell_type": "markdown",
      "source": "The sentences include some informal words and some unclear/meaningless punctuation. They should be replaced with sth making more sense to the sentences."
    },
    {
      "metadata": {
        "_uuid": "8f736f259edb767d770988cc05ea241b72d37e45"
      },
      "cell_type": "markdown",
      "source": "#### 2. Define preprocessing procedures"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": true,
        "_uuid": "2a92412b8b47472240d6ed10d54c68357144f30b"
      },
      "cell_type": "code",
      "source": "import re\ndef preprocess(string):\n    '''\n    :param string:\n    :return:\n    '''\n    string = string.lower()\n    string = re.sub(r'(\\\\\")', '', string)\n    string = re.sub(r'(\\\\r\\\\n)', ' ', string)\n    string = re.sub(r'(\\\\r)', ' ', string)\n    string = re.sub(r'(\\\\n)', ' ', string)\n    string = re.sub(r'(\\\\)', ' ', string)\n    string = re.sub(r'\\\\t', ' ', string)\n    string = re.sub(r'\\:', ' ', string)\n    string = re.sub(r'\\\"\\\"\\\"\\\"', ' ', string)\n    string = re.sub(r'_', ' ', string)\n    string = re.sub(r'\\+', ' ', string)\n    string = re.sub(r'\\=', ' ', string)\n    string = re.sub(r'\\-', ' ', string)\n    #sep numstring to \"num string\"\n    string = \" \".join(re.split(r'(\\d+)', string))\n    string = string.strip()\n    return string",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6a0cb6d970841352223d665fbef5563d0e32fef8"
      },
      "cell_type": "markdown",
      "source": "#### 3. Define tokenization procedures"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": true,
        "_uuid": "2315936eb3351438907784638c6c8292a871cec1"
      },
      "cell_type": "code",
      "source": "words_dict = dict()\n#for sentences checking\n#for i in range(20,50):\n#    print(df_train['project_title'][i])\n#tokenize_sentences(df_train['project_title'], words_dict, early_stopping = False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "dcdad22e23370b09f3ff2b71d488be4a612976d7"
      },
      "cell_type": "markdown",
      "source": "Before starting tokenizing, I find out that there are data differences in the datasets: Starting on May 17, 2016, the number of essays was reduced from 4 to 2, and the questions of the essays are different as well.\n\n    project_essay_1: \"Introduce us to your classroom\"\n    project_essay_2: \"Tell us more about your students\"\n    project_essay_3: \"Describe how your students will use the materials you're requesting\"\n    project_essay_4: \"Close by sharing why your project will make a difference\"\n    \n    Starting on May 17, 2016, the number of essays was reduced from 4 to 2, and the prompts for the first 2 essays were changed to the following:\n\n    project_essay_1: \"Describe your students: What makes your students special? Specific details about their background, your neighborhood, and your school are all helpful.\"\n    project_essay_2: \"About your project: How will these materials make a difference in your students' learning and improve their school lives?\"\n    \nI think the essay 1 and 2 before May 17, 2016, can be combined into the idea of essay 1 after May 17, 2016. Same idea for essay 3 and 4. So the sentences should get combination before tokenization. Also, to build a simpler model, I will try to combine all text data into one column and do it with LSTM or GRU model."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": true,
        "_uuid": "21e3784f29d4e36558b6432e2ef44e3a99348b0e"
      },
      "cell_type": "code",
      "source": "#combine the sentences\ndef combine_text(df):\n    try:\n        if len(df['text']) == len(df): return df\n    except: pass\n    df['text'] = \"\"\n    text_col  = ['project_title', 'project_essay_1', \n                 'project_essay_2', 'project_essay_3', \n                 'project_essay_4', 'project_resource_summary']\n        \n    #find nan values and apply fillna\n    df[text_col] = df[text_col].fillna('NANWORD')\n    df['text'] = df.apply(lambda x: \" \".join([str(x[col]).strip() for col in text_col]), axis=1)\n    #df.drop(['project_title', 'project_essay_1', \n    #         'project_essay_2', 'project_essay_3', \n    #         'project_essay_4', 'project_resource_summary'], axis = 1, inplace = True)\n    return df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "92bdd7355b3e2815723b033af6ce9e4623ac3084",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#df_train.drop('text', axis = 1, inplace = True)\ndf_train = combine_text(df_train)\ndf_train.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false,
        "_uuid": "8d798452e742699b31e579b42c88ac01d70adbfc"
      },
      "cell_type": "code",
      "source": "#print(df_train.iloc[0, -3])\n#print(df_train.iloc[0, -2])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": true,
        "_uuid": "e8c1ab507a901ca4990e9b0b5e92951f4537e949"
      },
      "cell_type": "code",
      "source": "def tokenize_sentences(sentences, words_dict, early_stopping = False):\n    '''\n    read sentences from the dataset and return tokenized_sentences and local words_dict\n    early_stopping is set to run small dataset to check how the tokenization goes\n    '''\n    tokenized_sentences = []\n    for sentence in tqdm.tqdm(sentences):\n        #check attribute first\n        if hasattr(sentence, \"decode\"): \n            sentence = sentence.decode(\"utf-8\")\n        #run preprocessing\n        sentence = preprocess(sentence)\n        #start tokenizing\n        tokens = nltk.tokenize.word_tokenize(sentence)\n        result = []\n        for word in tokens:\n            word = word.lower()\n            if word not in words_dict:\n                words_dict[word] = len(words_dict)\n            word_index = words_dict[word]\n            result.append(word_index)\n        tokenized_sentences.append(result)\n        if early_stopping:\n            print(sentence)\n            if len(tokenized_sentences) == 50:\n                return tokenized_sentences, words_dict\n    return tokenized_sentences, words_dict",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "80d1aef5f642bcf8624429f41d2bf586704fd743",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#start tokenization \ndf_train['tokenized_text'], words_dict = tokenize_sentences(df_train['text'], words_dict)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": true,
        "_uuid": "389e3021192fafd640bc2b15c5f3e568729db880"
      },
      "cell_type": "code",
      "source": "#count the sentence length for each project\ndf_train[\"tokenized_text_length\"] = df_train['tokenized_text'].apply(lambda x: len(x))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ed66642a082a52d3d08d768e4e1e978277938fdb"
      },
      "cell_type": "markdown",
      "source": "### Now we finish the tokenization on training set!\nSince I didn't see any huge differences in individual project title and essay, and all the texts should be viewed as a whole context item, I'm going to check whether length of the context have impacts on the approval. Also, I would like to check the length distribution and decide whether I should set the length maximum for the text data. If the differences of the text length are small, maybe I can apply CNN model and the idea of BoW(Bag-of-Words) to make the training phrase much faster."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "61add8d83c09cd333966f36f177c7a3a89c5341e",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "plt.hist(df_train[df_train[\"project_is_approved\"] == True][\"tokenized_text_length\"], \n         histtype = 'bar', bins=50, range = (150, 650),\n         label = \"Approved\",\n         color = 'red', stacked = True)\nplt.hist(df_train[df_train[\"project_is_approved\"] == False][\"tokenized_text_length\"], \n         histtype = 'bar', bins=50, range = (150, 650),\n         label = \"Not approved\",\n         color = 'green', stacked = True)\nplt.legend()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0d35142dd75e1e9364f371896f9360d4e2039dae",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "len(df_train[(df_train[\"project_is_approved\"] == True) & (df_train[\"tokenized_text_length\"]>= 630)])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "00c32ca7786f162ac517275c96b1eeacc6a54258"
      },
      "cell_type": "markdown",
      "source": "Well, the length doesn't mean much to the approvals, since both approved and unapproved have the same data distribution."
    },
    {
      "metadata": {
        "_uuid": "8fcb757b1ead09ed824946cabf6f4c6ef54b77f1"
      },
      "cell_type": "markdown",
      "source": "#### 4. Lookup the words_dict onto embedded_words_dict\nAlthough we have a huge pre-trained embedded word vectors, but we might not need them all, so now I would like to check how many words will be used in the dataset."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": true,
        "_uuid": "aed01b40c741110f32232889c44a2be3a30b7294"
      },
      "cell_type": "code",
      "source": "def clear_embedding_list(embedding_list, embedding_word_dict, words_dict):\n    '''\n    The dataset might not use all the pre-trained word vectors, we only need the words included in the dataset.\n    '''\n    cleared_embedding_list = []\n    cleared_embedding_word_dict = {}\n\n    for word in words_dict:\n        if word not in embedding_word_dict:\n            continue\n        word_id = embedding_word_dict[word]\n        row = embedding_list[word_id]\n        cleared_embedding_list.append(row)\n        cleared_embedding_word_dict[word] = len(cleared_embedding_word_dict)\n\n    return cleared_embedding_list, cleared_embedding_word_dict",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c56ba9b4151d6719860cf02c7cb11c4bf900a8fe",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "words_dict['nanword']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": true,
        "_uuid": "82e68af17fe94283ba400faea3503ca03b96b8c6"
      },
      "cell_type": "code",
      "source": "def convert_tokens_to_embedded(tokenized_sentences, words_to_ids, embedding_word_dict, \n                          sentences_length = 630, padding = embedding_word_dict['nanword'], \n                               early_stopping = False):\n    '''\n    tokenized_sentences and embedding_word_dict have different word ids, this function would try to look them up\n    Input: local tokenized sentence\n    Output: Lookup embedded tokenized sentence\n    '''\n    words_train = []\n    #loop all the sentences of the local dataset\n    for sentence in tokenized_sentences:\n        current_words = []\n        for word_index in sentence:\n            word = words_to_ids[word_index]\n            #why -1? -1 to get index and len(embedding_word_dict) = 400001, \n            #cleared_embedding_word_dict['unknownword'], cleared_embedding_word_dict['nanword']\n            #look up to embedding_word_dict to find the word index\n            word_id = embedding_word_dict.get(word, embedding_word_dict['unknownword'])\n            current_words.append(word_id)\n        #set maximum for sentence length\n        if len(current_words) >= sentences_length:\n            current_words = current_words[:sentences_length]\n        else:\n            #tokenized_sentences length is less than the setup sentence length => add NANWORD\n            current_words += [padding] * (sentences_length - len(current_words))\n        if early_stopping and len(words_train) >= 10:\n            break\n        words_train.append(current_words)\n    return words_train",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": true,
        "_uuid": "fd0438fe6736d3d94d54f388fe0111737a6bcbd9"
      },
      "cell_type": "code",
      "source": "words_to_ids = dict()\nfor key, value in words_dict.items():\n    words_to_ids[value] = key",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "99ddb7b29037ae46bd1401f5fc327530addafb9d"
      },
      "cell_type": "code",
      "source": "train_x_words = np.array(convert_tokens_to_embedded(df_train['tokenized_text'], \n                words_to_ids, embedding_word_dict, \n                sentences_length = 630))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dc4e65d5a3fdc2ff47c130b3ec65bde9628c2103",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#print(' '.join(words_to_ids[word] for word in df_train['tokenized_text'][0]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ce5c6365b7312a44151edf83e90012ef772ca64e"
      },
      "cell_type": "markdown",
      "source": "Note that lists in DataFrame are hard to convert the whole column without looping(I couldn't find any ways to fix that)."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "771210de3a74b95e1c82235c92c33ed87ebb471e",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#df_train.drop('lookup_tokenzied_sentence', axis = 1, inplace = True)\ndf_train['tokenized_text'].head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false,
        "_uuid": "bcf7c0f3b60b9c0e45c049f6c636cc9da6dd1089"
      },
      "cell_type": "code",
      "source": "#check code\n#\" \".join([embedding_ids_dict[word_idx] for word_idx in convert_tokens_to_ids(df_train['tokenized_text'], \n#                                      words_to_ids, embedding_word_dict, \n#                                      early_stopping = True)[0]])\n#Looks pretty good!",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "9f2fca46b5a0bac270477667de29f59a1db45760"
      },
      "cell_type": "markdown",
      "source": "Now we have got the embedded word vector lookup done, let's try to train the model next."
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "cd7285f1054198c3f301283d0e129e49bc0f7d8b"
      },
      "cell_type": "code",
      "source": "#df_train.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fefd1290cff4d3fe94e413b44a2917d6f77e68e4",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#drop some unused columns to release memory\ndf_train.drop(['project_title', 'project_essay_1', 'project_essay_2', 'project_essay_3', 'project_essay_4',\n              'project_resource_summary', 'text'], axis = 1, inplace = True)\ngc.collect()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5229f6d0ff2b1bdadb89d1895a41918d94b2dfa7"
      },
      "cell_type": "markdown",
      "source": "## Update: Add numerical data into the model and train an mixed model\nFrom the baseline model, we also see that the total price of the project and the number of previously posted projects can easily make a nice prediction. So I would like to add the numerical nodes into the last dense DNN model and train the model again to see if it can do better prediction than the first try (0.77703). Note to shrink the model computation and make the numerical data have more power on it, I adjust the second dense layer from 32 nodes down to 16."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d69ce91ff382c9270ce7fc6fc638ae78e5152b2e",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#note that price is only for one quantity of the resource\ndf_resources['total_price'] = df_resources['quantity'] * df_resources['price']\nsum_total_price = pd.DataFrame(df_resources.groupby('id').total_price.sum()).reset_index()\n#sum_total_price.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "6b0cd59711af981760dae038b96d798825999d22"
      },
      "cell_type": "code",
      "source": "#append resources items to train and test\ndf_train = pd.merge(df_train, sum_total_price, on='id')\ndf_test = pd.merge(df_test, sum_total_price, on = 'id')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "54557af3baa68115cd5f4456da31c864118b6f11",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "train_x_num = np.array(df_train[['total_price', 'teacher_number_of_previously_posted_projects', 'tokenized_text_length']])\n#import scaler\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n\ntrain_x_num = scaler.fit_transform(train_x_num)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "4abb2c2fa1f3264797cddb5de22da71c2adb51d5"
      },
      "cell_type": "markdown",
      "source": "### Train a GRU model"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "45e65c0cca54df205141a02e235c18f7dbea9aee",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#import modules from keras\nfrom keras.layers import Input, Dense, Embedding, Flatten, concatenate, Dropout, Convolution1D, \\\n                         GlobalMaxPool1D, GlobalAveragePooling1D, SpatialDropout1D, CuDNNGRU, Dropout,\\\n                         Bidirectional, GRU, BatchNormalization, AveragePooling1D, MaxPooling1D\nfrom keras.models import Model, Sequential\nfrom keras import optimizers",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": true,
        "_uuid": "7270cfd764fc338418aab69da7e9c19b04d67a29"
      },
      "cell_type": "code",
      "source": "def get_model(embedding_matrix = embedding_list, num_features = 3,\n              maxlen = 630, dropout_rate = 0.4, learning_rate = 5e-3,\n              recurrent_units = 64):\n    '''\n    embedding_matrix needs to be an array\n    '''\n    input_words = Input((maxlen, ))\n    \n    input_nums = Input((num_features, ))\n    #set dim for embedded word\n    embedding_dim = 300\n    max_features = len(embedding_matrix)\n    \n    x_words = Embedding(max_features, embedding_dim,\n                        weights = [embedding_matrix], \n                        input_length = maxlen,\n                        trainable=False)(input_words)\n    x_words = SpatialDropout1D(dropout_rate)(x_words)\n    \n    x_words1 = Bidirectional(GRU(recurrent_units, return_sequences=True))(x_words)\n    #set filters to be 128, kernel_size to be 5\n    x_words1 = Convolution1D(64, 5, activation=\"relu\")(x_words1)\n    x_words1_1 = MaxPooling1D(pool_size = 2, strides = 2, padding = 'same')(x_words1)\n    x_words1_1 = GlobalAveragePooling1D()(x_words1)\n    \n    x_words2 = Convolution1D(32, 5, activation=\"relu\")(x_words)\n    x_words2 = MaxPooling1D(2,padding='same')(x_words2)\n    x_words2 = Convolution1D(64, 5, activation=\"relu\")(x_words2)\n    x_words2_1 = MaxPooling1D(2,padding='same')(x_words2)\n    x_words2_1 = GlobalAveragePooling1D()(x_words2_1)\n    \n    x = concatenate([x_words1_1, x_words2_1])\n    #print(x.shape)\n    x = BatchNormalization()(x)\n    \n    dense_units = [64, 16]\n    #x = Flatten()(x)\n    x = Dropout(dropout_rate)(x)\n    x = Dense(dense_units[0], activation=\"relu\")(x)\n    x = Dropout(dropout_rate)(x)\n    x = Dense(dense_units[1], activation=\"relu\")(x)\n    \n    #join x_nums\n    x_nums = input_nums\n    #since x might have some gradient vanishing situations already, I would like to do batchnorm again before concatenate with x_nums\n    x = BatchNormalization()(x)\n    x = concatenate([x, x_nums])\n    predictions = Dense(1, activation=\"sigmoid\")(x)\n    model = Model(inputs=[input_words, input_nums], outputs=predictions)\n    model.compile(optimizer=optimizers.Adam(learning_rate, decay=1e-6),\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n    return model",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "973e97f7b1c997db8f00bb80fc882c355376ba4c"
      },
      "cell_type": "code",
      "source": "#del model\n#del get_model\n#gc.collect()\nmodel = get_model()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": true,
        "_uuid": "444adfa44578ee0a6ec1aeaec1745e7579b647af"
      },
      "cell_type": "code",
      "source": "from keras.callbacks import *\nfrom sklearn.metrics import roc_auc_score\nfile_path = 'best.h5'",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "382dcb21b22e389e84d0ea7e6851a8df455e0a0f"
      },
      "cell_type": "code",
      "source": "#test = np.array(train_x_words)\n#test.shape\n#train_x_words.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1aeeb9e0d0bff3400bc92632d20ed3861109466c",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True,\n                             save_weights_only=True, mode='min')\nearly = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=4)\nlr_reduced = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=1,\n                               epsilon=1e-4, mode='min')\n\ncallbacks_list = [checkpoint, early, lr_reduced]\n\nhistory = model.fit([train_x_words, train_x_num], \n                    df_train['project_is_approved'], validation_split=0.03,\n                    verbose=1, callbacks=callbacks_list, epochs=10, batch_size=256)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false,
        "_uuid": "7c5386722e88f00a4adec2ef0c035dcf0c020a45"
      },
      "cell_type": "markdown",
      "source": "### Test data preprocessing\n\nAfter done model training, now I'm going to to text lookup for the test dataset. Here are the procedures:\n    \n1. Do sentence combination and tokenization on test dataset\n2. Look up the tokenized words onto the words_dict\n3. Run prediction with the processed test data\n\nSince I train the NLP model with words appeared in the training dataset, the first idea come to me is that the test dataset may only check whether the word shows up in the training words_dict. But the word vectors are imported from pre-trained model, maybe it's better to refer directly to the embedded_words_dict when doing prediction."
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "08bec7f3d75821e801a006b4dac784724a932661"
      },
      "cell_type": "code",
      "source": "#del data only for training to release memory\ndel df_train\n#del cleared_embedding_word_dict\n#del cleared_embedding_list\ndel train_x_words\ngc.collect()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7d8218dc263c94d1fe1dd478a74fc08b31632711",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# test data preprocessing\ndf_test = combine_text(df_test)\n# drop other text columns directly\ndf_test.drop(['project_title', 'project_essay_1', 'project_essay_2', 'project_essay_3', 'project_essay_4',\n             'project_resource_summary'], axis = 1, inplace = True)\ngc.collect()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2d0a01ade2266b4e06174e363ad024c4f916bee3",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# test text tokenization\ndf_test['tokenized_text'], words_dict = tokenize_sentences(df_test['text'], words_dict)\ndf_test[\"tokenized_text_length\"] = df_test['tokenized_text'].apply(lambda x: len(x))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "b18505cf1467dab28c25c193baf99742541a7d07"
      },
      "cell_type": "code",
      "source": "# test_x_num\ntest_x_num = np.array(df_test[['total_price', 'teacher_number_of_previously_posted_projects', 'tokenized_text_length']])\ntest_x_num = scaler.transform(test_x_num)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "1cd8aad0893704bdd69d9e31929a3054668fc838"
      },
      "cell_type": "code",
      "source": "#words_to_ids = dict()\n#renew words_to_ids\nfor key, value in words_dict.items():\n    words_to_ids[value] = key",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "75fc48356d4c397e77947f4dc1431798b9feeef8"
      },
      "cell_type": "code",
      "source": "#test tokenized words vector lookup\ntest_x_words = np.array(convert_tokens_to_embedded(df_test['tokenized_text'], \n                words_to_ids, embedding_word_dict, \n                padding = embedding_word_dict['nanword'],\n                sentences_length = 630))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "61ca3d14648d9357eb594bb35c1658c81ae01faa"
      },
      "cell_type": "code",
      "source": "#run prediction\nmodel.load_weights(file_path)\npred_test = model.predict([test_x_words, test_x_num], batch_size=1024, verbose=1)\n\ndf_test[\"project_is_approved\"] = pred_test\n#submit our prediction!\ndf_test[['id', 'project_is_approved']].to_csv(\"submission.csv\", index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9804160e26ae913481c4b93e1e713ec421aaf6f6"
      },
      "cell_type": "markdown",
      "source": "#### Future goals:\n1. Add numerical data into the model and train an mixed model\n2. Experiment some hyperparameter tuning"
    },
    {
      "metadata": {
        "_uuid": "5596f3203d5394319894d22d03413475fd909dca"
      },
      "cell_type": "markdown",
      "source": "Reference:\n\nhttps://www.kaggle.com/fizzbuzz/beginner-s-guide-to-capsule-networks\n\nhttps://www.kaggle.com/fizzbuzz/the-all-in-one-model"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false,
        "_uuid": "758114eea6a949730dfb145921e330a8ff5043c7"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}