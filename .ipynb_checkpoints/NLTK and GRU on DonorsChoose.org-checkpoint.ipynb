{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After take a look into the data distribution of DonorsChoose.org, I found that I couldn't build a simple model that applied features such as grades, subject categories, states, and number of previously posted projects, to make precise prediction of whether the project would get approved or not. So now I'm going to dig into how the project title and project essays make people decide to support it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/KunWuYao/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#load modules\n",
    "import nltk #for tokenizing\n",
    "import tqdm\n",
    "#https://github.com/tqdm/tqdm 第三方進度條模組\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#PunktSentenceTokenizer\n",
    "nltk.download(\"punkt\")\n",
    "import zipfile\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load dataset\n",
    "def load_zip_data(file_path, file_name):\n",
    "    df = zipfile.ZipFile(file_path)\n",
    "    df = pd.read_csv(df.open(file_name))\n",
    "    return df\n",
    "df_train = load_zip_data('train.zip', 'train.csv')\n",
    "df_test = load_zip_data('test.zip', 'test.csv')\n",
    "df_resources = load_zip_data('resources.zip', 'resources.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data types and columns\n",
    "\n",
    "**Text**: project_title, project_essay_1, project_essay_2, project_essay_3, project_essay_4, project_resource_summary in **train/test**, and description in **resources**\n",
    "\n",
    "**Class/Label**: teacher_id, teacher_prefix, project_grade_category, project_subject_categories, project_subject_subcategories in **train/test**\n",
    "\n",
    "**Quantity**: quantity and price in **resources**, and teacher_number_of_previously_posted_projects in **train/test**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deal with Text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load the pre-trained embedded word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load word embedded vectors\n",
    "import os\n",
    "embedding_path = \"/Users/KunWuYao/GitHub/NLP_toolbox/gloVe/glove.6B/glove.6B.300d.txt\"\n",
    "\n",
    "def read_embedding_vec(embedding_file_path):\n",
    "    embedding_list = []\n",
    "    embedding_word_dict = dict()\n",
    "    \n",
    "    f = open(embedding_file_path)\n",
    "\n",
    "    for index, line in enumerate(f):\n",
    "        #the line contains target word and its word vector, so split it first\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        try:\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "        except:\n",
    "            continue\n",
    "        #save the word vec into a list\n",
    "        embedding_list.append(coefs)\n",
    "        #build a connection between word and its word id (0, 1, 2, ......)\n",
    "        embedding_word_dict[word] = len(embedding_word_dict)\n",
    "    f.close()\n",
    "    embedding_list = np.array(embedding_list)\n",
    "    return embedding_list, embedding_word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load embedding word vectors\n",
    "embedding_list, embedding_word_dict = read_embedding_vec(embedding_path)\n",
    "#embedding_list[:5], embedding_word_dict['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set word_vector for UNKNOWN_WORD and _NAN_\n",
    "try:\n",
    "    embedding_word_dict['nanword']\n",
    "except:\n",
    "    embedding_word_dict['nanword'] = len(embedding_word_dict)\n",
    "    embedding_list = np.append(embedding_list, [np.zeros_like(embedding_list[0,:])], axis = 0)\n",
    "try:\n",
    "    embedding_word_dict['unknownword']\n",
    "except:\n",
    "    embedding_word_dict['unknownword'] = len(embedding_word_dict)\n",
    "    embedding_list = np.append(embedding_list, [np.zeros_like(embedding_list[0,:])], axis = 0)\n",
    "#(embedding_word_dict['nanword'], embedding_word_dict['unknownword'], len(embedding_word_dict)), embedding_list[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#embedding_word_dict['3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have loaded the pre-trained word vectors, the next step is to transform our project titles and essays. Note that the word embedding does not include \"can't\", \"couldn't\", \"wouldn't\", and things like those, we need to separate the not things when doing tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentences include some informal words and some unclear/meaningless punctuation. They should be replaced with sth making more sense to the sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Define preprocessing procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocess(string):\n",
    "    '''\n",
    "    :param string:\n",
    "    :return:\n",
    "    '''\n",
    "    string = string.lower()\n",
    "    string = re.sub(r'(\\\\\")', '', string)\n",
    "    string = re.sub(r'(\\\\r\\\\n)', ' ', string)\n",
    "    string = re.sub(r'(\\\\r)', ' ', string)\n",
    "    string = re.sub(r'(\\\\n)', ' ', string)\n",
    "    string = re.sub(r'(\\\\)', ' ', string)\n",
    "    string = re.sub(r'\\\\t', ' ', string)\n",
    "    string = re.sub(r'\\:', ' ', string)\n",
    "    string = re.sub(r'\\\"\\\"\\\"\\\"', ' ', string)\n",
    "    string = re.sub(r'_', ' ', string)\n",
    "    string = re.sub(r'\\+', ' ', string)\n",
    "    string = re.sub(r'\\=', ' ', string)\n",
    "    string = re.sub(r'\\-', ' ', string)\n",
    "    #sep numstring to \"num string\"\n",
    "    string = \" \".join(re.split(r'(\\d+)', string))\n",
    "    string = string.strip()\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Define tokenization procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_dict = dict()\n",
    "#for sentences checking\n",
    "#for i in range(20,50):\n",
    "#    print(df_train['project_title'][i])\n",
    "#tokenize_sentences(df_train['project_title'], words_dict, early_stopping = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting tokenizing, I find out that there are data differences in the datasets: Starting on May 17, 2016, the number of essays was reduced from 4 to 2, and the questions of the essays are different as well.\n",
    "\n",
    "    project_essay_1: \"Introduce us to your classroom\"\n",
    "    project_essay_2: \"Tell us more about your students\"\n",
    "    project_essay_3: \"Describe how your students will use the materials you're requesting\"\n",
    "    project_essay_4: \"Close by sharing why your project will make a difference\"\n",
    "    \n",
    "    Starting on May 17, 2016, the number of essays was reduced from 4 to 2, and the prompts for the first 2 essays were changed to the following:\n",
    "\n",
    "    project_essay_1: \"Describe your students: What makes your students special? Specific details about their background, your neighborhood, and your school are all helpful.\"\n",
    "    project_essay_2: \"About your project: How will these materials make a difference in your students' learning and improve their school lives?\"\n",
    "    \n",
    "I think the essay 1 and 2 before May 17, 2016, can be combined into the idea of essay 1 after May 17, 2016. Same idea for essay 3 and 4. So the sentences should get combination before tokenization. Also, to build a simpler model, I will try to combine all text data into one column and do it with LSTM or GRU model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#combine the sentences\n",
    "def combine_text(df):\n",
    "    try:\n",
    "        if len(df['text']) == len(df): return df\n",
    "    except: pass\n",
    "    df['text'] = \"\"\n",
    "    text_col  = ['project_title', 'project_essay_1', \n",
    "                 'project_essay_2', 'project_essay_3', \n",
    "                 'project_essay_4', 'project_resource_summary']\n",
    "        \n",
    "    #find nan values and apply fillna\n",
    "    df[text_col] = df[text_col].fillna('NANWORD')\n",
    "    df['text'] = df.apply(lambda x: \" \".join([str(x[col]).strip() for col in text_col]), axis=1)\n",
    "    #df.drop(['project_title', 'project_essay_1', \n",
    "    #         'project_essay_2', 'project_essay_3', \n",
    "    #         'project_essay_4', 'project_resource_summary'], axis = 1, inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>teacher_id</th>\n",
       "      <th>teacher_prefix</th>\n",
       "      <th>school_state</th>\n",
       "      <th>project_submitted_datetime</th>\n",
       "      <th>project_grade_category</th>\n",
       "      <th>project_subject_categories</th>\n",
       "      <th>project_subject_subcategories</th>\n",
       "      <th>project_title</th>\n",
       "      <th>project_essay_1</th>\n",
       "      <th>project_essay_2</th>\n",
       "      <th>project_essay_3</th>\n",
       "      <th>project_essay_4</th>\n",
       "      <th>project_resource_summary</th>\n",
       "      <th>teacher_number_of_previously_posted_projects</th>\n",
       "      <th>project_is_approved</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p036502</td>\n",
       "      <td>484aaf11257089a66cfedc9461c6bd0a</td>\n",
       "      <td>Ms.</td>\n",
       "      <td>NV</td>\n",
       "      <td>2016-11-18 14:45:59</td>\n",
       "      <td>Grades PreK-2</td>\n",
       "      <td>Literacy &amp; Language</td>\n",
       "      <td>Literacy</td>\n",
       "      <td>Super Sight Word Centers</td>\n",
       "      <td>Most of my kindergarten students come from low...</td>\n",
       "      <td>I currently have a differentiated sight word c...</td>\n",
       "      <td>NANWORD</td>\n",
       "      <td>NANWORD</td>\n",
       "      <td>My students need 6 Ipod Nano's to create and d...</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>Super Sight Word Centers Most of my kindergart...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p039565</td>\n",
       "      <td>df72a3ba8089423fa8a94be88060f6ed</td>\n",
       "      <td>Mrs.</td>\n",
       "      <td>GA</td>\n",
       "      <td>2017-04-26 15:57:28</td>\n",
       "      <td>Grades 3-5</td>\n",
       "      <td>Music &amp; The Arts, Health &amp; Sports</td>\n",
       "      <td>Performing Arts, Team Sports</td>\n",
       "      <td>Keep Calm and Dance On</td>\n",
       "      <td>Our elementary school is a culturally rich sch...</td>\n",
       "      <td>We strive to provide our diverse population of...</td>\n",
       "      <td>NANWORD</td>\n",
       "      <td>NANWORD</td>\n",
       "      <td>My students need matching shirts to wear for d...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Keep Calm and Dance On Our elementary school i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p233823</td>\n",
       "      <td>a9b876a9252e08a55e3d894150f75ba3</td>\n",
       "      <td>Ms.</td>\n",
       "      <td>UT</td>\n",
       "      <td>2017-01-01 22:57:44</td>\n",
       "      <td>Grades 3-5</td>\n",
       "      <td>Math &amp; Science, Literacy &amp; Language</td>\n",
       "      <td>Applied Sciences, Literature &amp; Writing</td>\n",
       "      <td>Lets 3Doodle to Learn</td>\n",
       "      <td>Hello;\\r\\nMy name is Mrs. Brotherton. I teach ...</td>\n",
       "      <td>We are looking to add some 3Doodler to our cla...</td>\n",
       "      <td>NANWORD</td>\n",
       "      <td>NANWORD</td>\n",
       "      <td>My students need the 3doodler. We are an SEM s...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Lets 3Doodle to Learn Hello;\\r\\nMy name is Mrs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p185307</td>\n",
       "      <td>525fdbb6ec7f538a48beebaa0a51b24f</td>\n",
       "      <td>Mr.</td>\n",
       "      <td>NC</td>\n",
       "      <td>2016-08-12 15:42:11</td>\n",
       "      <td>Grades 3-5</td>\n",
       "      <td>Health &amp; Sports</td>\n",
       "      <td>Health &amp; Wellness</td>\n",
       "      <td>\\\"Kid Inspired\\\" Equipment to Increase Activit...</td>\n",
       "      <td>My students are the greatest students but are ...</td>\n",
       "      <td>The student's project which is totally \\\"kid-i...</td>\n",
       "      <td>NANWORD</td>\n",
       "      <td>NANWORD</td>\n",
       "      <td>My students need balls and other activity equi...</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>\\\"Kid Inspired\\\" Equipment to Increase Activit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p013780</td>\n",
       "      <td>a63b5547a7239eae4c1872670848e61a</td>\n",
       "      <td>Mr.</td>\n",
       "      <td>CA</td>\n",
       "      <td>2016-08-06 09:09:11</td>\n",
       "      <td>Grades 6-8</td>\n",
       "      <td>Health &amp; Sports</td>\n",
       "      <td>Health &amp; Wellness</td>\n",
       "      <td>We need clean water for our culinary arts class!</td>\n",
       "      <td>My students are athletes and students who are ...</td>\n",
       "      <td>For some reason in our kitchen the water comes...</td>\n",
       "      <td>NANWORD</td>\n",
       "      <td>NANWORD</td>\n",
       "      <td>My students need a water filtration system for...</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>We need clean water for our culinary arts clas...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                        teacher_id teacher_prefix school_state  \\\n",
       "0  p036502  484aaf11257089a66cfedc9461c6bd0a            Ms.           NV   \n",
       "1  p039565  df72a3ba8089423fa8a94be88060f6ed           Mrs.           GA   \n",
       "2  p233823  a9b876a9252e08a55e3d894150f75ba3            Ms.           UT   \n",
       "3  p185307  525fdbb6ec7f538a48beebaa0a51b24f            Mr.           NC   \n",
       "4  p013780  a63b5547a7239eae4c1872670848e61a            Mr.           CA   \n",
       "\n",
       "  project_submitted_datetime project_grade_category  \\\n",
       "0        2016-11-18 14:45:59          Grades PreK-2   \n",
       "1        2017-04-26 15:57:28             Grades 3-5   \n",
       "2        2017-01-01 22:57:44             Grades 3-5   \n",
       "3        2016-08-12 15:42:11             Grades 3-5   \n",
       "4        2016-08-06 09:09:11             Grades 6-8   \n",
       "\n",
       "            project_subject_categories  \\\n",
       "0                  Literacy & Language   \n",
       "1    Music & The Arts, Health & Sports   \n",
       "2  Math & Science, Literacy & Language   \n",
       "3                      Health & Sports   \n",
       "4                      Health & Sports   \n",
       "\n",
       "            project_subject_subcategories  \\\n",
       "0                                Literacy   \n",
       "1            Performing Arts, Team Sports   \n",
       "2  Applied Sciences, Literature & Writing   \n",
       "3                       Health & Wellness   \n",
       "4                       Health & Wellness   \n",
       "\n",
       "                                       project_title  \\\n",
       "0                           Super Sight Word Centers   \n",
       "1                             Keep Calm and Dance On   \n",
       "2                              Lets 3Doodle to Learn   \n",
       "3  \\\"Kid Inspired\\\" Equipment to Increase Activit...   \n",
       "4   We need clean water for our culinary arts class!   \n",
       "\n",
       "                                     project_essay_1  \\\n",
       "0  Most of my kindergarten students come from low...   \n",
       "1  Our elementary school is a culturally rich sch...   \n",
       "2  Hello;\\r\\nMy name is Mrs. Brotherton. I teach ...   \n",
       "3  My students are the greatest students but are ...   \n",
       "4  My students are athletes and students who are ...   \n",
       "\n",
       "                                     project_essay_2 project_essay_3  \\\n",
       "0  I currently have a differentiated sight word c...         NANWORD   \n",
       "1  We strive to provide our diverse population of...         NANWORD   \n",
       "2  We are looking to add some 3Doodler to our cla...         NANWORD   \n",
       "3  The student's project which is totally \\\"kid-i...         NANWORD   \n",
       "4  For some reason in our kitchen the water comes...         NANWORD   \n",
       "\n",
       "  project_essay_4                           project_resource_summary  \\\n",
       "0         NANWORD  My students need 6 Ipod Nano's to create and d...   \n",
       "1         NANWORD  My students need matching shirts to wear for d...   \n",
       "2         NANWORD  My students need the 3doodler. We are an SEM s...   \n",
       "3         NANWORD  My students need balls and other activity equi...   \n",
       "4         NANWORD  My students need a water filtration system for...   \n",
       "\n",
       "   teacher_number_of_previously_posted_projects  project_is_approved  \\\n",
       "0                                            26                    1   \n",
       "1                                             1                    0   \n",
       "2                                             5                    1   \n",
       "3                                            16                    0   \n",
       "4                                            42                    1   \n",
       "\n",
       "                                                text  \n",
       "0  Super Sight Word Centers Most of my kindergart...  \n",
       "1  Keep Calm and Dance On Our elementary school i...  \n",
       "2  Lets 3Doodle to Learn Hello;\\r\\nMy name is Mrs...  \n",
       "3  \\\"Kid Inspired\\\" Equipment to Increase Activit...  \n",
       "4  We need clean water for our culinary arts clas...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_train.drop('text', axis = 1, inplace = True)\n",
    "df_train = combine_text(df_train)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(df_train.iloc[0, -3])\n",
    "#print(df_train.iloc[0, -2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_sentences(sentences, words_dict, early_stopping = False):\n",
    "    '''\n",
    "    read sentences from the dataset and return tokenized_sentences and local words_dict\n",
    "    early_stopping is set to run small dataset to check how the tokenization goes\n",
    "    '''\n",
    "    tokenized_sentences = []\n",
    "    for sentence in tqdm.tqdm(sentences):\n",
    "        #check attribute first\n",
    "        if hasattr(sentence, \"decode\"): \n",
    "            sentence = sentence.decode(\"utf-8\")\n",
    "        #run preprocessing\n",
    "        sentence = preprocess(sentence)\n",
    "        #start tokenizing\n",
    "        tokens = nltk.tokenize.word_tokenize(sentence)\n",
    "        result = []\n",
    "        for word in tokens:\n",
    "            word = word.lower()\n",
    "            if word not in words_dict:\n",
    "                words_dict[word] = len(words_dict)\n",
    "            word_index = words_dict[word]\n",
    "            result.append(word_index)\n",
    "        tokenized_sentences.append(result)\n",
    "        if early_stopping:\n",
    "            print(sentence)\n",
    "            if len(tokenized_sentences) == 50:\n",
    "                return tokenized_sentences, words_dict\n",
    "    return tokenized_sentences, words_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start tokenization \n",
    "df_train['tokenized_text'], words_dict = tokenize_sentences(df_train['text'], words_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#count the sentence length for each project\n",
    "df_train[\"tokenized_text_length\"] = df_train['tokenized_text'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we finish the tokenization on training set!\n",
    "Since I didn't see any huge differences in individual project title and essay, and all the texts should be viewed as a whole context item, I'm going to check whether length of the context have impacts on the approval. Also, I would like to check the length distribution and decide whether I should set the length maximum for the text data. If the differences of the text length are small, maybe I can apply CNN model and the idea of BoW(Bag-of-Words) to make the training phrase much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x128e09a20>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEQoHERGJUDiIiEiEwkFERCL+P80xHgdZyvjyAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x128e09860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df_train[df_train[\"project_is_approved\"] == True][\"tokenized_text_length\"], \n",
    "         histtype = 'bar', bins=50, range = (150, 650),\n",
    "         label = \"Approved\",\n",
    "         color = 'red', stacked = True)\n",
    "plt.hist(df_train[df_train[\"project_is_approved\"] == False][\"tokenized_text_length\"], \n",
    "         histtype = 'bar', bins=50, range = (150, 650),\n",
    "         label = \"Not approved\",\n",
    "         color = 'green', stacked = True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train[(df_train[\"project_is_approved\"] == True) & (df_train[\"tokenized_text_length\"]>= 630)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, the length doesn't mean much to the approvals, since both approved and unapproved have the same data distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Lookup the words_dict onto embedded_words_dict\n",
    "Although we have a huge pre-trained embedded word vectors, but we might not need them all, so now I would like to check how many words will be used in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def clear_embedding_list(embedding_list, embedding_word_dict, words_dict):\n",
    "#    '''\n",
    "#    The dataset might not use all the pre-trained word vectors, we only need the words included in the dataset.\n",
    "#    '''\n",
    "#    cleared_embedding_list = []\n",
    "#    cleared_embedding_word_dict = {}\n",
    "#\n",
    "#    for word in words_dict:\n",
    "#        if word not in embedding_word_dict:\n",
    "#            continue\n",
    "#        word_id = embedding_word_dict[word]\n",
    "#        row = embedding_list[word_id]\n",
    "#        cleared_embedding_list.append(row)\n",
    "#        cleared_embedding_word_dict[word] = len(cleared_embedding_word_dict)\n",
    "#\n",
    "#    return cleared_embedding_list, cleared_embedding_word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleared_embedding_list, cleared_embedding_word_dict = clear_embedding_list(embedding_list, \n",
    "#                                                                           embedding_word_dict, \n",
    "#                                                                           words_dict)\n",
    "#print(len(cleared_embedding_word_dict), cleared_embedding_word_dict['nanword'])\n",
    "#print(cleared_embedding_word_dict['unknownword']) #didn't lookup for unknown_word yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#try:\n",
    "#    cleared_embedding_word_dict['unknownword']\n",
    "#except:\n",
    "#    cleared_embedding_word_dict['unknownword'] = len(cleared_embedding_word_dict)\n",
    "#    cleared_embedding_list = np.append(cleared_embedding_list, \n",
    "#                                       [np.zeros_like(cleared_embedding_list[0])], axis = 0)\n",
    "#try:\n",
    "#    cleared_embedding_word_dict['nanword']\n",
    "#except:\n",
    "#    cleared_embedding_word_dict['nanword'] = len(cleared_embedding_word_dict)\n",
    "#    cleared_embedding_list = np.append(cleared_embedding_list, \n",
    "#                                       [np.zeros_like(cleared_embedding_list[0])], axis = 0)\n",
    "#cleared_embedding_list.shape, (cleared_embedding_word_dict['unknownword'], len(cleared_embedding_word_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_tokens_to_embedded(tokenized_sentences, words_to_ids, embedding_word_dict, \n",
    "                          sentences_length = 630, padding = embedding_word_dict['nanword'], \n",
    "                               early_stopping = False):\n",
    "    '''\n",
    "    tokenized_sentences and embedding_word_dict have different word ids, this function would try to look them up\n",
    "    Input: local tokenized sentence\n",
    "    Output: Lookup embedded tokenized sentence\n",
    "    '''\n",
    "    words_train = []\n",
    "    #loop all the sentences of the local dataset\n",
    "    for sentence in tokenized_sentences:\n",
    "        current_words = []\n",
    "        for word_index in sentence:\n",
    "            word = words_to_ids[word_index]\n",
    "            #why -1? -1 to get index and len(embedding_word_dict) = 400001, \n",
    "            #embedding_word_dict['unknownword'], embedding_word_dict['nanword']\n",
    "            #look up to embedding_word_dict to find the word index\n",
    "            word_id = embedding_word_dict.get(word, embedding_word_dict['unknownword'])\n",
    "            current_words.append(word_id)\n",
    "        #set maximum for sentence length\n",
    "        if len(current_words) >= sentences_length:\n",
    "            current_words = current_words[:sentences_length]\n",
    "        else:\n",
    "            #tokenized_sentences length is less than the setup sentence length => add NANWORD\n",
    "            current_words += [padding] * (sentences_length - len(current_words))\n",
    "        if early_stopping and len(words_train) >= 10:\n",
    "            break\n",
    "        words_train.append(current_words)\n",
    "    return words_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_to_ids = dict()\n",
    "for key, value in words_dict.items():\n",
    "    words_to_ids[value] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x_words = np.array(convert_tokens_to_embedded(df_train['tokenized_text'], \n",
    "                words_to_ids, embedding_word_dict, \n",
    "                sentences_length = 630))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(' '.join(words_to_ids[word] for word in df_train['tokenized_text'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that lists in DataFrame are hard to convert the whole column without looping(I couldn't find any ways to fix that)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...\n",
       "1    [173, 174, 14, 175, 124, 117, 176, 24, 41, 65,...\n",
       "2    [275, 276, 277, 23, 60, 278, 279, 6, 280, 41, ...\n",
       "3    [357, 358, 359, 23, 360, 120, 14, 361, 362, 36...\n",
       "4    [83, 152, 442, 443, 36, 117, 444, 445, 299, 10...\n",
       "Name: tokenized_text, dtype: object"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_train.drop('lookup_tokenzied_sentence', axis = 1, inplace = True)\n",
    "df_train['tokenized_text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#check code\n",
    "#\" \".join([embedding_ids_dict[word_idx] for word_idx in convert_tokens_to_ids(df_train['tokenized_text'], \n",
    "#                                      words_to_ids, embedding_word_dict, \n",
    "#                                      early_stopping = True)[0]])\n",
    "#Looks pretty good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#embedding_ids_dict = dict()\n",
    "#for key, value in embedding_word_dict.items():\n",
    "#    embedding_ids_dict[value] = key\n",
    "#del embedding_ids_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we have got the embedded word vector lookup done, let's try to train the model next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>teacher_id</th>\n",
       "      <th>teacher_prefix</th>\n",
       "      <th>school_state</th>\n",
       "      <th>project_submitted_datetime</th>\n",
       "      <th>project_grade_category</th>\n",
       "      <th>project_subject_categories</th>\n",
       "      <th>project_subject_subcategories</th>\n",
       "      <th>project_title</th>\n",
       "      <th>project_essay_1</th>\n",
       "      <th>project_essay_2</th>\n",
       "      <th>project_essay_3</th>\n",
       "      <th>project_essay_4</th>\n",
       "      <th>project_resource_summary</th>\n",
       "      <th>teacher_number_of_previously_posted_projects</th>\n",
       "      <th>project_is_approved</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>tokenized_text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p036502</td>\n",
       "      <td>484aaf11257089a66cfedc9461c6bd0a</td>\n",
       "      <td>Ms.</td>\n",
       "      <td>NV</td>\n",
       "      <td>2016-11-18 14:45:59</td>\n",
       "      <td>Grades PreK-2</td>\n",
       "      <td>Literacy &amp; Language</td>\n",
       "      <td>Literacy</td>\n",
       "      <td>Super Sight Word Centers</td>\n",
       "      <td>Most of my kindergarten students come from low...</td>\n",
       "      <td>I currently have a differentiated sight word c...</td>\n",
       "      <td>NANWORD</td>\n",
       "      <td>NANWORD</td>\n",
       "      <td>My students need 6 Ipod Nano's to create and d...</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>Super Sight Word Centers Most of my kindergart...</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p039565</td>\n",
       "      <td>df72a3ba8089423fa8a94be88060f6ed</td>\n",
       "      <td>Mrs.</td>\n",
       "      <td>GA</td>\n",
       "      <td>2017-04-26 15:57:28</td>\n",
       "      <td>Grades 3-5</td>\n",
       "      <td>Music &amp; The Arts, Health &amp; Sports</td>\n",
       "      <td>Performing Arts, Team Sports</td>\n",
       "      <td>Keep Calm and Dance On</td>\n",
       "      <td>Our elementary school is a culturally rich sch...</td>\n",
       "      <td>We strive to provide our diverse population of...</td>\n",
       "      <td>NANWORD</td>\n",
       "      <td>NANWORD</td>\n",
       "      <td>My students need matching shirts to wear for d...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Keep Calm and Dance On Our elementary school i...</td>\n",
       "      <td>[173, 174, 14, 175, 124, 117, 176, 24, 41, 65,...</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p233823</td>\n",
       "      <td>a9b876a9252e08a55e3d894150f75ba3</td>\n",
       "      <td>Ms.</td>\n",
       "      <td>UT</td>\n",
       "      <td>2017-01-01 22:57:44</td>\n",
       "      <td>Grades 3-5</td>\n",
       "      <td>Math &amp; Science, Literacy &amp; Language</td>\n",
       "      <td>Applied Sciences, Literature &amp; Writing</td>\n",
       "      <td>Lets 3Doodle to Learn</td>\n",
       "      <td>Hello;\\r\\nMy name is Mrs. Brotherton. I teach ...</td>\n",
       "      <td>We are looking to add some 3Doodler to our cla...</td>\n",
       "      <td>NANWORD</td>\n",
       "      <td>NANWORD</td>\n",
       "      <td>My students need the 3doodler. We are an SEM s...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Lets 3Doodle to Learn Hello;\\r\\nMy name is Mrs...</td>\n",
       "      <td>[275, 276, 277, 23, 60, 278, 279, 6, 280, 41, ...</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p185307</td>\n",
       "      <td>525fdbb6ec7f538a48beebaa0a51b24f</td>\n",
       "      <td>Mr.</td>\n",
       "      <td>NC</td>\n",
       "      <td>2016-08-12 15:42:11</td>\n",
       "      <td>Grades 3-5</td>\n",
       "      <td>Health &amp; Sports</td>\n",
       "      <td>Health &amp; Wellness</td>\n",
       "      <td>\\\"Kid Inspired\\\" Equipment to Increase Activit...</td>\n",
       "      <td>My students are the greatest students but are ...</td>\n",
       "      <td>The student's project which is totally \\\"kid-i...</td>\n",
       "      <td>NANWORD</td>\n",
       "      <td>NANWORD</td>\n",
       "      <td>My students need balls and other activity equi...</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>\\\"Kid Inspired\\\" Equipment to Increase Activit...</td>\n",
       "      <td>[357, 358, 359, 23, 360, 120, 14, 361, 362, 36...</td>\n",
       "      <td>489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p013780</td>\n",
       "      <td>a63b5547a7239eae4c1872670848e61a</td>\n",
       "      <td>Mr.</td>\n",
       "      <td>CA</td>\n",
       "      <td>2016-08-06 09:09:11</td>\n",
       "      <td>Grades 6-8</td>\n",
       "      <td>Health &amp; Sports</td>\n",
       "      <td>Health &amp; Wellness</td>\n",
       "      <td>We need clean water for our culinary arts class!</td>\n",
       "      <td>My students are athletes and students who are ...</td>\n",
       "      <td>For some reason in our kitchen the water comes...</td>\n",
       "      <td>NANWORD</td>\n",
       "      <td>NANWORD</td>\n",
       "      <td>My students need a water filtration system for...</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>We need clean water for our culinary arts clas...</td>\n",
       "      <td>[83, 152, 442, 443, 36, 117, 444, 445, 299, 10...</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                        teacher_id teacher_prefix school_state  \\\n",
       "0  p036502  484aaf11257089a66cfedc9461c6bd0a            Ms.           NV   \n",
       "1  p039565  df72a3ba8089423fa8a94be88060f6ed           Mrs.           GA   \n",
       "2  p233823  a9b876a9252e08a55e3d894150f75ba3            Ms.           UT   \n",
       "3  p185307  525fdbb6ec7f538a48beebaa0a51b24f            Mr.           NC   \n",
       "4  p013780  a63b5547a7239eae4c1872670848e61a            Mr.           CA   \n",
       "\n",
       "  project_submitted_datetime project_grade_category  \\\n",
       "0        2016-11-18 14:45:59          Grades PreK-2   \n",
       "1        2017-04-26 15:57:28             Grades 3-5   \n",
       "2        2017-01-01 22:57:44             Grades 3-5   \n",
       "3        2016-08-12 15:42:11             Grades 3-5   \n",
       "4        2016-08-06 09:09:11             Grades 6-8   \n",
       "\n",
       "            project_subject_categories  \\\n",
       "0                  Literacy & Language   \n",
       "1    Music & The Arts, Health & Sports   \n",
       "2  Math & Science, Literacy & Language   \n",
       "3                      Health & Sports   \n",
       "4                      Health & Sports   \n",
       "\n",
       "            project_subject_subcategories  \\\n",
       "0                                Literacy   \n",
       "1            Performing Arts, Team Sports   \n",
       "2  Applied Sciences, Literature & Writing   \n",
       "3                       Health & Wellness   \n",
       "4                       Health & Wellness   \n",
       "\n",
       "                                       project_title  \\\n",
       "0                           Super Sight Word Centers   \n",
       "1                             Keep Calm and Dance On   \n",
       "2                              Lets 3Doodle to Learn   \n",
       "3  \\\"Kid Inspired\\\" Equipment to Increase Activit...   \n",
       "4   We need clean water for our culinary arts class!   \n",
       "\n",
       "                                     project_essay_1  \\\n",
       "0  Most of my kindergarten students come from low...   \n",
       "1  Our elementary school is a culturally rich sch...   \n",
       "2  Hello;\\r\\nMy name is Mrs. Brotherton. I teach ...   \n",
       "3  My students are the greatest students but are ...   \n",
       "4  My students are athletes and students who are ...   \n",
       "\n",
       "                                     project_essay_2 project_essay_3  \\\n",
       "0  I currently have a differentiated sight word c...         NANWORD   \n",
       "1  We strive to provide our diverse population of...         NANWORD   \n",
       "2  We are looking to add some 3Doodler to our cla...         NANWORD   \n",
       "3  The student's project which is totally \\\"kid-i...         NANWORD   \n",
       "4  For some reason in our kitchen the water comes...         NANWORD   \n",
       "\n",
       "  project_essay_4                           project_resource_summary  \\\n",
       "0         NANWORD  My students need 6 Ipod Nano's to create and d...   \n",
       "1         NANWORD  My students need matching shirts to wear for d...   \n",
       "2         NANWORD  My students need the 3doodler. We are an SEM s...   \n",
       "3         NANWORD  My students need balls and other activity equi...   \n",
       "4         NANWORD  My students need a water filtration system for...   \n",
       "\n",
       "   teacher_number_of_previously_posted_projects  project_is_approved  \\\n",
       "0                                            26                    1   \n",
       "1                                             1                    0   \n",
       "2                                             5                    1   \n",
       "3                                            16                    0   \n",
       "4                                            42                    1   \n",
       "\n",
       "                                                text  \\\n",
       "0  Super Sight Word Centers Most of my kindergart...   \n",
       "1  Keep Calm and Dance On Our elementary school i...   \n",
       "2  Lets 3Doodle to Learn Hello;\\r\\nMy name is Mrs...   \n",
       "3  \\\"Kid Inspired\\\" Equipment to Increase Activit...   \n",
       "4  We need clean water for our culinary arts clas...   \n",
       "\n",
       "                                      tokenized_text  tokenized_text_length  \n",
       "0  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...                    369  \n",
       "1  [173, 174, 14, 175, 124, 117, 176, 24, 41, 65,...                    235  \n",
       "2  [275, 276, 277, 23, 60, 278, 279, 6, 280, 41, ...                    306  \n",
       "3  [357, 358, 359, 23, 360, 120, 14, 361, 362, 36...                    489  \n",
       "4  [83, 152, 442, 443, 36, 117, 444, 445, 299, 10...                    225  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102635"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop some unused columns to release memory\n",
    "df_train.drop(['project_title', 'project_essay_1', 'project_essay_2', 'project_essay_3', 'project_essay_4',\n",
    "              'project_resource_summary', 'text'], axis = 1, inplace = True)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Train a GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import modules from keras\n",
    "from keras.layers import Input, Dense, Embedding, Flatten, concatenate, Dropout, Convolution1D, \\\n",
    "                         GlobalMaxPool1D, GlobalAveragePooling1D, SpatialDropout1D, CuDNNGRU, Dropout,\\\n",
    "                         Bidirectional, GRU, BatchNormalization, AveragePooling1D, MaxPooling1D\n",
    "from keras.models import Model, Sequential\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model(embedding_matrix = cleared_embedding_list, \n",
    "              maxlen = 630, dropout_rate = 0.4, learning_rate = 5e-3,\n",
    "              recurrent_units = 64):\n",
    "    '''\n",
    "    embedding_matrix needs to be an array\n",
    "    '''\n",
    "    input_words = Input((maxlen, ))\n",
    "    #set dim for embedded word\n",
    "    embedding_dim = 300\n",
    "    max_features = len(embedding_matrix)\n",
    "    \n",
    "    x_words = Embedding(max_features, embedding_dim,\n",
    "                        weights = [embedding_matrix], \n",
    "                        input_length = maxlen,\n",
    "                        trainable=False)(input_words)\n",
    "    x_words = SpatialDropout1D(dropout_rate)(x_words)\n",
    "    \n",
    "    x_words1 = Bidirectional(GRU(recurrent_units, return_sequences=True))(x_words)\n",
    "    #set filters to be 128, kernel_size to be 5\n",
    "    x_words1 = Convolution1D(64, 5, activation=\"relu\")(x_words1)\n",
    "    x_words1_1 = MaxPooling1D(pool_size = 2, strides = 2, padding = 'same')(x_words1)\n",
    "    x_words1_1 = GlobalAveragePooling1D()(x_words1)\n",
    "    \n",
    "    x_words2 = Convolution1D(64, 5, activation=\"relu\")(x_words)\n",
    "    x_words2 = MaxPooling1D(2,padding='same')(x_words2)\n",
    "    x_words2 = Convolution1D(128, 5, activation=\"relu\")(x_words2)\n",
    "    x_words2_1 = MaxPooling1D(2,padding='same')(x_words2)\n",
    "    x_words2_1 = GlobalAveragePooling1D()(x_words2_1)\n",
    "    \n",
    "    x = concatenate([x_words1_1, x_words2_1])\n",
    "    #print(x.shape)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    dense_units = [64, 32]\n",
    "    #x = Flatten()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = Dense(dense_units[0], activation=\"relu\")(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = Dense(dense_units[1], activation=\"relu\")(x)\n",
    "    predictions = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=[input_words], outputs=predictions)\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate, decay=1e-6),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#del model\n",
    "#del get_model\n",
    "#gc.collect()\n",
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import *\n",
    "from sklearn.metrics import roc_auc_score\n",
    "file_path = 'best.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 176617 samples, validate on 5463 samples\n",
      "Epoch 1/5\n",
      "176384/176617 [============================>.] - ETA: 41s - loss: 0.3949 - acc: 0.8485 \n",
      "Epoch 00001: val_loss improved from inf to 0.39645, saving model to best.h5\n",
      "176617/176617 [==============================] - 32762s 185ms/step - loss: 0.3949 - acc: 0.8485 - val_loss: 0.3965 - val_acc: 0.8612\n",
      "Epoch 2/5\n",
      "  6400/176617 [>.............................] - ETA: 25:51:02 - loss: 0.3703 - acc: 0.8564"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True,\n",
    "                             save_weights_only=True, mode='min')\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=4)\n",
    "lr_reduced = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=1,\n",
    "                               epsilon=1e-4, mode='min')\n",
    "\n",
    "callbacks_list = [checkpoint, early, lr_reduced]\n",
    "\n",
    "history = model.fit(train_x_words, \n",
    "                    df_train['project_is_approved'], validation_split=0.03,\n",
    "                    verbose=1, callbacks=callbacks_list, epochs=5, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Test data preprocessing\n",
    "\n",
    "After done model training, now I'm going to to text lookup for the test dataset. Here are the procedures:\n",
    "    \n",
    "1. Do sentence combination and tokenization on test dataset\n",
    "2. Look up the tokenized words onto the words_dict\n",
    "3. Run prediction with the processed test data\n",
    "\n",
    "Since I train the NLP model with words appeared in the training dataset, the first idea come to me is that the test dataset may only check whether the word shows up in the training words_dict. But the word vectors are imported from pre-trained model, maybe it's better to refer directly to the embedded_words_dict when doing prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#del data only for training to release memory\n",
    "del df_train\n",
    "#del cleared_embedding_word_dict\n",
    "#del cleared_embedding_list\n",
    "del train_x_words\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data preprocessing\n",
    "df_test = combine_text(df_test)\n",
    "# drop other text columns directly\n",
    "df_test.drop(['project_title', 'project_essay_1', 'project_essay_2', 'project_essay_3', 'project_essay_4',\n",
    "             'project_resource_summary'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test text tokenization\n",
    "df_test['tokenized_text'], words_dict = tokenize_sentences(df_test['text'], words_dict)\n",
    "df_test[\"tokenized_text_length\"] = df_test['tokenized_text'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#words_to_ids = dict()\n",
    "#renew words_to_ids\n",
    "for key, value in words_dict.items():\n",
    "    words_to_ids[value] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test tokenized words vector lookup\n",
    "test_x_words = np.array(convert_tokens_to_embedded(df_test['tokenized_text'], \n",
    "                words_to_ids, embedding_word_dict, \n",
    "                padding = embedding_word_dict['nanword'],\n",
    "                sentences_length = 630))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#run prediction\n",
    "model.load_weights(file_path)\n",
    "pred_test = model.predict(test_x_words, batch_size=1024, verbose=1)\n",
    "\n",
    "df_test[\"project_is_approved\"] = pred_test\n",
    "#submit our prediction!\n",
    "df_test[['id', 'project_is_approved']].to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "\n",
    "https://www.kaggle.com/fizzbuzz/beginner-s-guide-to-capsule-networks\n",
    "\n",
    "https://www.kaggle.com/fizzbuzz/the-all-in-one-model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
